{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 3 Supervised Learning Capstone\n",
    "\n",
    "This capstone project is fosused on prediction of the precense of heart disease in individuals using data obtained from traditional diagnostic tests. The data is provided by the Cleveland Clinic Database. The data was provided by [Kaggel](https://www.kaggle.com/) the HEART DISEASE webpage is at this link  [HD_Dataset](https://www.kaggle.com/ronitf/heart-disease-uci).\n",
    "\n",
    "#### Data Overview\n",
    "The original data set contained personal information and more features than is provide to the public. There were originally 76 attributes measured in the original data. The data has been scrubbed of personal identifiers and the attributes reduced to 14. Of the 14 attributes there are both catigorical and continous varibles. The target feature is binary to indicate either the precence or abcense of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "from mlflow import sklearn as mlflow_sklearn\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import catboost\n",
    "import lightgbm \n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "from collections import defaultdict\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion, _fit_transform_one, _transform_one\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "### Data Set Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and look at first rows\n",
    "df = pd.read_csv('heart.csv')\n",
    "print(df.head(3))\n",
    "print('')\n",
    "\n",
    "# General stats on data\n",
    "print(df.describe())\n",
    "print('')\n",
    "\n",
    "# Target bias\n",
    "print('Data bias')\n",
    "target_sum = df['target'].count()\n",
    "target_count = df['target'].value_counts()\n",
    "percent_pos = round(target_count[0] / target_sum *100,1)\n",
    "percent_neg = 100 - percent_pos\n",
    "print('Total number targets is {}, individuals with heart disease present is {}% positve and {}% negitive'.\n",
    "      format(target_sum,str(percent_pos), str(percent_neg) ))\n",
    "\n",
    "# Gender bias\n",
    "gender_sum = df['sex'].count()\n",
    "gender_count = df['sex'].value_counts()\n",
    "percent_male = round(gender_count[1] / gender_sum *100,1)\n",
    "percent_female = round(100 - percent_male,1)\n",
    "print('Of the {} participents, {}% are Male and {}% are Female'.\n",
    "      format(gender_sum,str(percent_male), str(percent_female) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data for contionous or catagorical type\n",
    "df.hist(bins=25, grid=False, figsize=(12,10), color='#86bf91', zorder=2, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Reorder columns to group varibles by type [continous, catagorical, binary]\n",
    "cols = list(df.columns.values)\n",
    "new_index = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'cp',  'restecg', 'slope', 'ca', 'thal', 'exang', 'fbs', 'sex', 'target']\n",
    "df = df.reindex(columns=new_index)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_index = [ 'cp',  'restecg', 'slope', 'ca', 'thal']\n",
    "# df[category_index] = df[category_index].astype('category')\n",
    "\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age_female = df.age[df['sex']==0]\n",
    "age_male = df.age[df['sex']==1]\n",
    "df_age = df.copy()\n",
    "df_age = df_age.groupby('sex')\n",
    "print(df_age['age'].describe())\n",
    "# Plot Data\n",
    "age_male.hist(bins=30)\n",
    "age_female.hist(bins=30)\n",
    "plt.title('Histogram of Age Distribution')\n",
    "plt.legend(['male','female'])\n",
    "plt.xlabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.trestbps,  df.age)\n",
    "plt.scatter(df.chol,  df.age)\n",
    "plt.scatter(df.thalach,  df.age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasFeatureUnion(FeatureUnion):\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self._validate_transformers()\n",
    "        result = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_transform_one)(trans, X, y, weight,\n",
    "                                        **fit_params)\n",
    "            for name, trans, weight in self._iter())\n",
    "\n",
    "        if not result:\n",
    "            # All transformers are None\n",
    "            return np.zeros((X.shape[0], 0))\n",
    "        Xs, transformers = zip(*result)\n",
    "        self._update_transformer_list(transformers)\n",
    "        if any(sparse.issparse(f) for f in Xs):\n",
    "            Xs = sparse.hstack(Xs).tocsr()\n",
    "        else:\n",
    "            Xs = self.merge_dataframes_by_column(Xs)\n",
    "        return Xs\n",
    "\n",
    "    def merge_dataframes_by_column(self, Xs):\n",
    "        return pd.concat(Xs, axis=\"columns\", copy=False)\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xs = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_transform_one)(trans, X, None, weight)\n",
    "            for name, trans, weight in self._iter())\n",
    "        if not Xs:\n",
    "            # All transformers are None\n",
    "            return np.zeros((X.shape[0], 0))\n",
    "        if any(sparse.issparse(f) for f in Xs):\n",
    "            Xs = sparse.hstack(Xs).tocsr()\n",
    "        else:\n",
    "            Xs = self.merge_dataframes_by_column(Xs)\n",
    "        return Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _name_estimators(estimators):\n",
    "    \"\"\"Generate names for estimators.\"\"\"\n",
    "\n",
    "    names = [type(estimator).__name__.lower() for estimator in estimators]\n",
    "    namecount = defaultdict(int)\n",
    "    for est, name in zip(estimators, names):\n",
    "        namecount[name] += 1\n",
    "\n",
    "    for k, v in list(six.iteritems(namecount)):\n",
    "        if v == 1:\n",
    "            del namecount[k]\n",
    "\n",
    "    for i in reversed(range(len(estimators))):\n",
    "        name = names[i]\n",
    "        if name in namecount:\n",
    "            names[i] += \"-%d\" % namecount[name]\n",
    "            namecount[name] -= 1\n",
    "\n",
    "    return list(zip(names, estimators))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pandas_union(*transformers, **kwargs):\n",
    "    n_jobs = kwargs.pop('n_jobs', None)\n",
    "    if kwargs:\n",
    "        raise TypeError('Unknown keyword arguments: \"{}\"'\n",
    "                        .format(list(kwargs.keys())[0]))\n",
    "    return PandasFeatureUnion(_name_estimators(transformers), n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalEncoderPandas(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.transformers = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            self.transformers[column] = ce.OrdinalEncoder(return_df=False, handle_unknown=\"impute\").fit(X[[column]])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.drop(list(set(X.columns) - set(self.columns)), axis=1)\n",
    "        for column in self.columns:\n",
    "            X[column] = self.transformers[column].transform(X[[column]])\n",
    "            X[column] = X[column].apply(lambda x: x if x else -1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoderPandas(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.transformers = {}\n",
    "        self.feature_names = {}\n",
    "        self.feature_names_all = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            self.transformers[column] = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(X[[column]])\n",
    "            features = [f\"{column}_{i}\" for i in self.transformers[column].get_feature_names()]\n",
    "            self.feature_names[column] = features\n",
    "            for feature in features:\n",
    "                self.feature_names_all.append(feature)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        ohe_df_list = []\n",
    "        for column in self.columns:\n",
    "            ohe_df = pd.DataFrame(self.transformers[column].transform(X[[column]]))\n",
    "            feature_names = [f\"{column}_{i}\" for i in self.transformers[column].get_feature_names()]\n",
    "            ohe_df.columns = self.feature_names[column]\n",
    "            ohe_df_list.append(ohe_df)\n",
    "        ohe_df_concat = pd.concat(ohe_df_list, axis=1)\n",
    "        return ohe_df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumn(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, columns, no_drops):\n",
    "        self.columns = columns\n",
    "        self.no_drops = no_drops\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            if column in X.columns:\n",
    "                drop_together = False\n",
    "                if self.no_drops:\n",
    "                    for no_drop in self.no_drops:\n",
    "                        if column == no_drop and self.no_drops[no_drop] not in X.columns:\n",
    "                            drop_together = True\n",
    "                if not drop_together:\n",
    "                    X = X.drop(columns=column)\n",
    "            else:\n",
    "                print(f\"Drop Warning: Column {column} not in X\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeColumnType(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, types):\n",
    "        self.types = types\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for column in self.types.keys():\n",
    "            if column in X.columns:\n",
    "                X[column] = X[column].astype(self.types[column])\n",
    "            else:\n",
    "                print(f\"Change Warning: Column {column} not in X\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- XGBoost/Random Forest requires OHE of categorical variables unless categorial variable is ordinal\n",
    "- XGBoost/LightGBM/Catboost supports missing variables, but RandomForests do not (most models in sklearn does not support missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features 5 to 9 (4 feature in total) categorical type (non-binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(df_samples.columns[5:9])\n",
    "target = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_features:\n",
    "    df_samples[i] = abs(df_samples[i]).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples[target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Models that does not support categorical variables\n",
    "\n",
    "Need to use OHE, only when you know the data is not ordinal, otherwise you can use ordinal encoding\n",
    "\n",
    "XGBoost, RandomForest (CART can handle categorical, but RF does not have this implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ohe = make_pipeline(\n",
    "    make_pandas_union(\n",
    "        DropColumn(columns=categorical_features, no_drops=None),\n",
    "        make_pipeline(\n",
    "            ChangeColumnType(types={i: str for i in categorical_features}),\n",
    "            OneHotEncoderPandas(columns=categorical_features)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ohe.fit_transform(df_samples).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_columns = list(set(pipe_ohe.fit_transform(df_samples).columns) - set([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for models that support categorical variables\n",
    "\n",
    "LightGBM, CatBoost can handle categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cat = make_pipeline(\n",
    "    make_pandas_union(\n",
    "        DropColumn(columns=categorical_features, no_drops=None),\n",
    "        OrdinalEncoderPandas(columns=categorical_features)\n",
    "    ),\n",
    "    ChangeColumnType(types={i: \"category\" for i in categorical_features}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cat.fit_transform(df_samples).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_ohe = pipe_ohe.fit_transform(df_samples)\n",
    "train_ohe, test_ohe = train_test_split(df_samples_ohe, test_size=0.2, stratify=df_samples_ohe[target], random_state=0)\n",
    "train_ohe, valid_ohe = train_test_split(train_ohe, test_size=0.2, stratify=train_ohe[target], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samples_cat = pipe_cat.fit_transform(df_samples)\n",
    "train_cat, test_cat = train_test_split(df_samples_cat, test_size=0.2, stratify=df_samples_cat[target], random_state=0)\n",
    "train_cat, valid_cat = train_test_split(train_cat, test_size=0.2, stratify=train_cat[target], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features=None,\n",
    "    n_estimators=20,\n",
    "    max_depth=4,\n",
    "    random_state=0,\n",
    "    n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "# Use validation set to modify hyperparameters\n",
    "print(accuracy_score(valid_ohe[target], rf_classifier.predict(valid_ohe[ohe_columns])))\n",
    "# Use testing set to evaluate final performance\n",
    "print(accuracy_score(test_ohe[target], rf_classifier.predict(test_ohe[ohe_columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgboost.XGBClassifier(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.008,\n",
    "    n_estimators=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "# Use validation set to modify hyperparameters\n",
    "print(accuracy_score(valid_ohe[target], xgb_classifier.predict(valid_ohe[ohe_columns])))\n",
    "# Use testing set to evaluate final performance\n",
    "print(accuracy_score(test_ohe[target], xgb_classifier.predict(test_ohe[ohe_columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_classifier = lightgbm.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    categorical_features=\"auto\",\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "# Use validation set to modify hyperparameters\n",
    "print(accuracy_score(valid_ohe[target], lgb_classifier.predict(valid_ohe[ohe_columns])))\n",
    "# Use testing set to evaluate final performance\n",
    "print(accuracy_score(test_ohe[target], lgb_classifier.predict(test_ohe[ohe_columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_classifier = catboost.CatBoostClassifier(\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=200,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "# Use validation set to modify hyperparameters\n",
    "print(accuracy_score(valid_ohe[target], cat_classifier.predict(valid_ohe[ohe_columns])))\n",
    "# Use testing set to evaluate final performance\n",
    "print(accuracy_score(test_ohe[target], cat_classifier.predict(test_ohe[ohe_columns])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Training/Testing/Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Random Forest\"):\n",
    "    criterion = \"entropy\"\n",
    "    max_features = None\n",
    "    n_estimators = 20\n",
    "    max_depth = 4\n",
    "    \n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        criterion=criterion,\n",
    "        max_features=max_features,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=0,\n",
    "        n_jobs=4)\n",
    "    rf_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "    \n",
    "    valid_accuracy = accuracy_score(valid_ohe[target], rf_classifier.predict(valid_ohe[ohe_columns]))\n",
    "    test_accuracy = accuracy_score(test_ohe[target], rf_classifier.predict(test_ohe[ohe_columns]))\n",
    "    \n",
    "    mlflow.log_param(\"criterion\", criterion)\n",
    "    mlflow.log_param(\"max_features\", max_features)\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    mlflow.log_metric(\"valid_accuracy\", valid_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    mlflow_sklearn.log_model(rf_classifier, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"XGBoost\"):\n",
    "    n_estimators = 200\n",
    "    max_depth = 4\n",
    "    learning_rate = 0.008\n",
    "    \n",
    "    xgb_classifier = xgboost.XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators\n",
    "    )\n",
    "    xgb_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "    \n",
    "    valid_accuracy = accuracy_score(valid_ohe[target], xgb_classifier.predict(valid_ohe[ohe_columns]))\n",
    "    test_accuracy = accuracy_score(test_ohe[target], xgb_classifier.predict(test_ohe[ohe_columns]))\n",
    "    \n",
    "    mlflow.log_param(\"criterion\", criterion)\n",
    "    mlflow.log_param(\"max_features\", max_features)\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    mlflow.log_metric(\"valid_accuracy\", valid_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    mlflow_sklearn.log_model(xgb_classifier, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"LightGBM\"):\n",
    "    objective = \"binary\"\n",
    "    categorical_features = \"auto\"\n",
    "    n_estimators = 200\n",
    "    max_depth = 4\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    lgb_classifier = lightgbm.LGBMClassifier(\n",
    "        objective=objective,\n",
    "        categorical_features=categorical_features,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators\n",
    "    )\n",
    "    lgb_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "    \n",
    "    valid_accuracy = accuracy_score(valid_ohe[target], lgb_classifier.predict(valid_ohe[ohe_columns]))\n",
    "    test_accuracy = accuracy_score(test_ohe[target], lgb_classifier.predict(test_ohe[ohe_columns]))\n",
    "    \n",
    "    mlflow.log_param(\"criterion\", criterion)\n",
    "    mlflow.log_param(\"max_features\", max_features)\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    mlflow.log_metric(\"valid_accuracy\", valid_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    mlflow_sklearn.log_model(lgb_classifier, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"CatBoost\"):\n",
    "    n_estimators = 200\n",
    "    max_depth = 8\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    cat_classifier = catboost.CatBoostClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        verbose=0\n",
    "    )\n",
    "    cat_classifier.fit(X=train_ohe[ohe_columns], y=train_ohe[target])\n",
    "    \n",
    "    valid_accuracy = accuracy_score(valid_ohe[target], cat_classifier.predict(valid_ohe[ohe_columns]))\n",
    "    test_accuracy = accuracy_score(test_ohe[target], cat_classifier.predict(test_ohe[ohe_columns]))\n",
    "    \n",
    "    mlflow.log_param(\"criterion\", criterion)\n",
    "    mlflow.log_param(\"max_features\", max_features)\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    \n",
    "    mlflow.log_metric(\"valid_accuracy\", valid_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "\n",
    "    mlflow_sklearn.log_model(cat_classifier, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing/Validation with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing with CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing with CV with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With ML-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
